{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial6: Building a more Complex Model with Self-Attention Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "\n",
    "from FlexMol.dataset.loader import load_DAVIS\n",
    "from FlexMol.encoder import FlexMol\n",
    "from FlexMol.task import BinaryTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DAVIS dataset\n",
    "# We are using a subset of the data (first 20 rows) for demonstration purposes\n",
    "train_df = load_DAVIS(\"data/DAVIS/train.txt\").head(20)\n",
    "val_df = load_DAVIS(\"data/DAVIS/val.txt\").head(20)\n",
    "test_df = load_DAVIS(\"data/DAVIS/test.txt\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FlexMol instance\n",
    "FM = FlexMol()\n",
    "\n",
    "# Initialize drug encoder: GCN\n",
    "# Setting output features to 128\n",
    "drug_encoder = FM.init_drug_encoder(\"GCN\", output_feats=128)\n",
    "\n",
    "# Initialize protein pocket encoder: PocketDC\n",
    "# Disable pooling  and the output shape will be 30 * 128\n",
    "pocket_encoder = FM.init_prot_encoder(\"PocketDC\", pdb=True, data_dir=\"data/DAVIS/pdb/\", num_pockets=30, output_feats=128, pooling=False)\n",
    "\n",
    "# Initialize protein encoder: GCN_ESM\n",
    "# Setting output features to 128\n",
    "protein_encoder = FM.init_prot_encoder(\"GCN_ESM\", pdb=True, hidden_feats=[128, 128, 128], data_dir=\"data/DAVIS/pdb/\", output_feats=128)\n",
    "\n",
    "# Set up self-attention interaction layer\n",
    "# Stack the encoders and apply self-attention interaction\n",
    "# output shape will be 32 * 128\n",
    "interaction_output = FM.set_interaction(FM.stack([drug_encoder, pocket_encoder, protein_encoder]), \"self_attention\")\n",
    "\n",
    "# Select and flatten the drug and protein outputs that encapsulate information about the pockets\n",
    "drug_final = FM.flatten(FM.select(interaction_output, index_start = 0))\n",
    "protein_final = FM.flatten(FM.select(interaction_output, index_start = 31))\n",
    "\n",
    "# Concatenate the final drug and protein outputs and apply MLP\n",
    "final_output = FM.apply_mlp(FM.cat([drug_final, protein_final]), hidden_layers=[512, 512, 256], head=1)\n",
    "\n",
    "# Build the model\n",
    "FM.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BinaryTrainer(\n",
    "    FM, \n",
    "    task=\"DTI\", \n",
    "    early_stopping=\"roc-auc\", \n",
    "    test_metrics=[\"roc-auc\", \"pr-auc\"], \n",
    "    device=\"cpu\", \n",
    "    epochs=25, \n",
    "    patience=6, \n",
    "    lr=0.0001, \n",
    "    batch_size=32, \n",
    ")\n",
    "\n",
    "# Prepare the datasets for training, validation, and testing\n",
    "train_data, val_data, test_data = trainer.prepare_datasets(train_df=train_df, val_df=val_df, test_df=test_df)\n",
    "\n",
    "# Train the model\n",
    "trainer.train(train_data, val_data)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
